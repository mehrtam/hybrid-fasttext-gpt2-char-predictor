# -*- coding: utf-8 -*-
"""shekspiere_GP2_fasttext.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PNYV0vhnNcSXXhQstmibPEU_2eCmkwI2
"""

from google.colab import drive
drive.mount('/content/drive')

# Define paths in Google Drive
drive_path = "/content/drive/MyDrive/FastText_Shakespeare"
model_path = f"{drive_path}/fasttext_shakespeare_quantized.ftz"
dataset_path = f"{drive_path}/char_dataset_smart.txt"
shakespeare_text_path = f"{drive_path}/shakespeare.txt"

!pip install datasets

import os
import requests

# Define paths
drive_path = "/content/drive/MyDrive/FastText_Shakespeare"
shakespeare_text_path = f"{drive_path}/shakespeare.txt"

# Ensure the directory exists
os.makedirs(drive_path, exist_ok=True)

# Check if dataset is already downloaded
if not os.path.exists(shakespeare_text_path):
    print("Downloading Shakespeare dataset...")
    url = "https://www.gutenberg.org/files/100/100-0.txt"
    response = requests.get(url)

    # Save file locally in Google Drive
    with open(shakespeare_text_path, "w", encoding="utf-8") as f:
        f.write(response.text)

    print(" Shakespeare dataset downloaded and saved!")
else:
    print(" Shakespeare dataset already exists in Google Drive.")

import random

def preprocess_text_for_fasttext(file_path, output_file, keep_percentage=0.5):
    """Processes text into FastText training format if not already created."""
    if os.path.exists(output_file):
        print(f" Processed dataset already exists at {output_file}")
        return

    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()

    text = text.replace("\n", " ")  # Remove newlines
    dataset = []

    for i in range(len(text) - 10):
        input_seq = text[i:i+10]
        next_char = text[i+10]

        # Hard Negative Sampling: Keep diverse samples
        if random.random() < keep_percentage or next_char in "?!,.":
            dataset.append(f"__label__{next_char} {input_seq}")

    # Save to file
    with open(output_file, "w", encoding="utf-8") as f:
        for line in dataset:
            f.write(line + "\n")

    print(f" Dataset saved as {output_file} with {len(dataset)} samples.")

# Process dataset only if needed
preprocess_text_for_fasttext(shakespeare_text_path, dataset_path, keep_percentage=0.5)

!pip install fasttext
import fasttext

# Train FastText model only if it does not exist
if not os.path.exists(model_path):
    print("Training FastText model...")
    model_ft = fasttext.train_supervised(
        input=dataset_path,
        epoch=400,
        lr=0.8,
        lrUpdateRate=400,
        wordNgrams=4,
        thread=8
    )

    # Apply quantization
    model_ft.quantize(input=dataset_path, retrain=True)

    # Save model in Google Drive
    model_ft.save_model(model_path)
    print(f" Model trained and saved at {model_path}")
else:
    print(f" Pre-trained model found at {model_path}, loading...")
    model_ft = fasttext.load_model(model_path)

import fasttext

if os.path.exists(model_path):
    print("Loading pre-trained FastText model...")
    model_ft = fasttext.load_model(model_path)
else:
    print(" Model not found! Run the training step first.")

from datasets import load_dataset

# Load Wikipedia dataset
dataset = load_dataset("wikipedia", "20220301.en", split="train", trust_remote_code=True)

# Extract text
wiki_texts = [item["text"] for item in dataset.select(range(100000))]  # Use 100K samples

# Save for FastText
with open("/content/wiki_text.txt", "w", encoding="utf-8") as f:
    for text in wiki_texts:
        f.write(text.replace("\n", " ") + "\n")

import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load GPT-2 model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model_gpt2 = GPT2LMHeadModel.from_pretrained("gpt2")
model_gpt2.eval()

def hybrid_predict_next_char(input_text, threshold=0.5):
    """
    Predicts the next character using fastText first.
    If fastText is uncertain, fall back to GPT-2.
    """
    # FastText prediction
    labels, confidence = model_ft.predict(input_text, k=3)
    predicted_chars = [label.replace("__label__", "") for label in labels]

    # If fastText confidence is low, use GPT-2
    if confidence[0] < threshold:
        input_ids = tokenizer.encode(input_text, return_tensors="pt")
        with torch.no_grad():
            output = model_gpt2(input_ids)
        logits = output.logits[:, -1, :]  # Last token logits
        predicted_token_id = torch.argmax(logits, dim=-1).item()
        predicted_chars = tokenizer.decode(predicted_token_id)

    return predicted_chars , confidence

# Interactive typing loop with Ctrl+C handling
try:
    while True:
        print("\nType something (press Ctrl+C to exit):")  # Correct placement
        user_input = input("Type: ")
        predicted_chars, confidences = hybrid_predict_next_char(user_input)
        print(f"Predicted next characters: {predicted_chars} (Confidence: {confidences})")

except KeyboardInterrupt:
    print("\n\nExiting program. See you next time!")

import time

# Updated test samples (random, diverse cases)
test_samples = [
    ("Machine learning is powe", "r"),  # AI-related
    ("The weather today is su", "n"),  # Everyday phrase
    ("Coding in Python is fu", "n"),  # Programming-related
    ("I love eating pizza wi", "t"),  # Casual sentence
    ("The mitochondrion is the powe", "r"),  # Scientific
    ("Can you solve this puz", "z"),  # Conversational
    ("Artificial Intelligence is chang", "i"),  # AI topic
    ("Please bring me a cup of co", "f"),  # Everyday scenario
    ("He is a great football pl", "a"),  # Sports-related
    ("Music makes me feel ha", "p")  # Emotional phrase
]

# Metrics Tracking
correct_top1 = 0
correct_top3 = 0
total_samples = len(test_samples)
total_time = 0

# Evaluate Model
for input_text, true_next_char in test_samples:
    start_time = time.time()

    predicted_chars, confidences = hybrid_predict_next_char(input_text)

    end_time = time.time()
    total_time += (end_time - start_time)

    # Top-1 Accuracy
    if predicted_chars[0] == true_next_char:
        correct_top1 += 1

    # Top-3 Accuracy
    if true_next_char in predicted_chars:
        correct_top3 += 1

# Calculate Metrics
top1_accuracy = (correct_top1 / total_samples) * 100
top3_accuracy = (correct_top3 / total_samples) * 100
average_latency = total_time / total_samples

# Print Results
print("\n=== Model Performance Metrics ===")
print(f"Top-1 Accuracy: {top1_accuracy:.2f}%")
print(f"Top-3 Accuracy: {top3_accuracy:.2f}%")
print(f"Average Prediction Time: {average_latency:.5f} seconds")

import fasttext
import torch
import time
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from datasets import load_dataset

# Step 1: Download and Prepare Wikipedia Dataset
def prepare_wikipedia_dataset():
    """
    Downloads Wikipedia dataset and prepares text data for FastText training.
    """
    print(" Downloading Wikipedia dataset...")
    dataset = load_dataset("wikipedia", "20220301.en", split="train", trust_remote_code=True)

    wiki_texts = [item["text"] for item in dataset.select(range(100000))]  # Use 100K samples

    with open("/content/wiki_text.txt", "w", encoding="utf-8") as f:
        for text in wiki_texts:
            f.write(text.replace("\n", " ") + "\n")

    print("Wikipedia dataset prepared!")

# Step 2: Train FastText Model on Wikipedia Data
def train_fasttext():
    """
    Trains a character-level FastText model on Wikipedia text.
    """
    print(" Training FastText model...")
    model_ft = fasttext.train_supervised(
        input="/content/wiki_text.txt",
        epoch=50,  # More training for better accuracy
        lr=1.5,  # Higher learning rate
        minn=2, maxn=5,  # Subword embeddings for character-level training
        wordNgrams=5,  # Use longer n-grams
        bucket=500000,  # More subword representations
        thread=8  # Speed up training
    )

    model_ft.save_model("/content/fasttext_wikipedia.bin")
    print("FastText model trained and saved!")

# Step 3: Load Trained Models (FastText + GPT-2)
def load_models():
    """
    Loads FastText and GPT-2 models for hybrid next-character prediction.
    """
    print(" Loading models...")
    model_ft = fasttext.load_model("/content/fasttext_wikipedia.bin")

    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model_gpt2 = GPT2LMHeadModel.from_pretrained("gpt2")
    model_gpt2.eval()

    print(" Models loaded!")
    return model_ft, tokenizer, model_gpt2

# Step 4: Hybrid Prediction (FastText + GPT-2)
def hybrid_predict_next_char(input_text, model_ft, tokenizer, model_gpt2, threshold=0.4):
    """
    Predicts the next character using FastText first.
    If FastText confidence is low, fall back to GPT-2.
    """
    labels, confidences = model_ft.predict(input_text, k=1)
    predicted_char = labels[0].replace("__label__", "")
    confidence = confidences[0]

    if confidence < threshold:  # If FastText is uncertain, use GPT-2
        input_ids = tokenizer.encode(input_text, return_tensors="pt")
        with torch.no_grad():
            output = model_gpt2(input_ids)
        logits = output.logits[:, -1, :]
        predicted_token_id = torch.argmax(logits, dim=-1).item()
        predicted_char = tokenizer.decode(predicted_token_id)

    return predicted_char, confidence

# Step 5: Test Model Performance
def evaluate_model(model_ft, tokenizer, model_gpt2):
    """
    Evaluates the model on random test sentences.
    """
    test_samples = [
        ("Machine learning is powe", "r"),
        ("The weather today is su", "n"),
        ("Coding in Python is fu", "n"),
        ("I love eating pizza wi", "t"),
        ("The mitochondrion is the powe", "r"),
        ("Can you solve this puz", "z"),
        ("Artificial Intelligence is chang", "i"),
        ("Please bring me a cup of co", "f"),
        ("He is a great football pl", "a"),
        ("Music makes me feel ha", "p")
    ]

    correct_top1, correct_top3, total_samples, total_time = 0, 0, len(test_samples), 0

    for input_text, true_next_char in test_samples:
        start_time = time.time()

        predicted_char, confidence = hybrid_predict_next_char(input_text, model_ft, tokenizer, model_gpt2)

        end_time = time.time()
        total_time += (end_time - start_time)

        if predicted_char == true_next_char:
            correct_top1 += 1
        if true_next_char in predicted_char:
            correct_top3 += 1

    # Calculate Metrics
    top1_accuracy = (correct_top1 / total_samples) * 100
    top3_accuracy = (correct_top3 / total_samples) * 100
    average_latency = total_time / total_samples

    print("\n=== Model Performance Metrics ===")
    print(f" Top-1 Accuracy: {top1_accuracy:.2f}%")
    print(f" Top-3 Accuracy: {top3_accuracy:.2f}%")
    print(f" Average Prediction Time: {average_latency:.5f} seconds")

# Step 6: Interactive Typing with Real-Time Predictions
def interactive_typing(model_ft, tokenizer, model_gpt2):
    """
    Runs an interactive typing mode where the user can type text and get predictions.
    """
    try:
        while True:
            print("\nType something (press Ctrl+C to exit):")
            user_input = input("Type: ")
            predicted_char, confidence = hybrid_predict_next_char(user_input, model_ft, tokenizer, model_gpt2)
            print(f"Predicted next character: {predicted_char} (Confidence: {confidence:.2f})")

    except KeyboardInterrupt:
        print("\n\n Exiting program. See you next time!")

# Step 7: Run the Entire Pipeline
if __name__ == "__main__":
    prepare_wikipedia_dataset()  # Download Wikipedia dataset
    train_fasttext()  # Train FastText on Wikipedia
    model_ft, tokenizer, model_gpt2 = load_models()  # Load FastText & GPT-2

    evaluate_model(model_ft, tokenizer, model_gpt2)  # Evaluate model
    interactive_typing(model_ft, tokenizer, model_gpt2)  # Start interactive mode

